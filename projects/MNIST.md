# MNIST Neural Network Classifier
Tech Stack: Python, NumPy, Pandas, Jupyter Notebook

## Overview
Built a custom neural network from scratch to classify handwritten digits from the MNIST dataset, achieving 94% accuracy. Implemented both forward and backward propagation using matrix operations and gradient-based optimization to update weights efficiently.

## Main Motivation
I wanted to understand the mathematics behind neural networks and LLMs, focusing on how data flows, learns, and converges through layers. This project helped me grasp activation functions, gradient descent, and the balance between training and testing performance.

## Key Takeaways
- Forward & Backward propogation
- Activation Functions (ReLU, Sigmoid) and their effects on learning
-Turning hyperparameters to study convergence and stability
